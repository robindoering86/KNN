{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm is very simple to implement, as it does not need to be trained. The training phase merely stores the training data. For each test point, we calculate the distance of that data point to every existing data point and find the $K$ closest ones. What we return is the the most common amongst the top k classification nearest to the test point. Here's the pseudocode for _K_ Nearest Neighbors:\n",
    "\n",
    "```\n",
    "kNN:\n",
    "\n",
    "    Learn:\n",
    "        Store training set T to X_train: X_train <-- T\n",
    "\n",
    "\n",
    "    Predict:\n",
    "        for every point xp in X_predict:\n",
    "            for every point x in X_train:\n",
    "                calculate the distance d in D between x and xp\n",
    "            sort D in increasing order\n",
    "            take the \"k\" items in X_train with the smallest distances to x\n",
    "            return the majority class among these k items\n",
    "```\n",
    "\n",
    "Note that for large data sets, the algorithm can take very long to classify because it has to calculate the distance between the test point and evey other point in the data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics - Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, KNN assigns a class to the test point based on the majority class of  KK  nearest neighbours. In general, euclidean distance is used to find nearest neighbours, but other distance metrics can also be used.\n",
    "\n",
    "As the dimensionality of the feature space increases, the euclidean distance often becomes problematic due to the curse of dimensionality (discussed later).\n",
    "\n",
    "In such cases, alternative vector-based similarity measures (dot product, cosine similarity, etc) are used to find the nearest neighbours. This transforms the original metric space into one more amicable to point-to-point measurements.\n",
    "\n",
    "Another distance measure that you might consider is Mahalanobis distance. Here's the wikipedia page on Mahalanobis distance. https://en.wikipedia.org/wiki/Mahalanobis_distance Mahalanobis distance attempts to weight features according to their probabilities. On some data sets that may be important.\n",
    "\n",
    "In general, it's probably a good idea to normalize the data at a minimum. Here's a link to the scikit learn scaling package. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html . You have to be a little circumspect about employing any technique where the answers change with scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(vec1, vec2, c=2, verbose=False):\n",
    "    if not len(vec1) == len(vec2):\n",
    "        print('vec1 and vec2 need to be of same dimension')\n",
    "        return 0\n",
    "    if verbose == True:\n",
    "        dm = 'Manhattan' if c == 1 else 'Euclidean' if c == 2 else 'Minkowski'\n",
    "        print('{} distance between vec1 and vec2:'.format(dm))\n",
    "    return np.power(sum([np.abs(vec1[i] - vec2[i])**c for i in range(len(vec1))]), 1/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class kNN():\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.k = 1\n",
    "\n",
    "    # Learn:\n",
    "    # Store training set T to X_train: X_train <-- T\n",
    "    def fit(self, X_train, y_train, k=1):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.k = k\n",
    "        \n",
    "    def predict(self, X_test, regr=False):\n",
    "        y_pred = []\n",
    "        \n",
    "        # for every point xp in X_predict:\n",
    "        for el_p in X_test:\n",
    "            dst_list = []\n",
    "\n",
    "            # for every point x in X_train:\n",
    "            for idx, el_t in enumerate(self.X_train):\n",
    "                \n",
    "                # calculate the distance d in D between el_p and el_t\n",
    "                dist = distance(el_p, el_t, c=2)\n",
    "                dst_list.append((dist, idx))\n",
    "\n",
    "            # sort dst_list in increasing order \n",
    "            k_dist = sorted(dst_list)[:self.k]\n",
    "            \n",
    "            # take the \"k\" items in y_train with the smallest distances to x\n",
    "            k_dist2 = [self.y_train[x[1]] for x in k_dist]\n",
    "            \n",
    "            # return the majority class among these k items\n",
    "            if regr == False:\n",
    "                y_pred.append(mode(k_dist2))\n",
    "            if regr == True:\n",
    "                y_pred.append(sum(k_dist2)/len(k_dist2))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import iris dataset\n",
    "df = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the species str label as integer using sklearn LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df['species'].unique())\n",
    "df['species'] = le.transform(df['species'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = kNN()\n",
    "knn3.fit(X_train.values, y_train.values, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.94      1.00      0.97        15\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn3.predict(X_test.values)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
